"""
Spring Boot ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸
backup í´ë”ì˜ ë¡œê·¸ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ì´ìƒ íŒ¨í„´ì„ íƒì§€í•©ë‹ˆë‹¤.
"""

import re
import os
import glob
import pickle
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
import warnings
warnings.filterwarnings('ignore')


class SpringBootLogParser:
    """Spring Boot ë¡œê·¸ íŒŒì„œ"""
    
    # Spring Boot ë¡œê·¸ íŒ¨í„´: 2025-07-02 15:59:36.514  INFO 12185 --- [           main] k.r.b.f.c.Application : Starting...
    LOG_PATTERN = re.compile(
        r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\.\d{3})\s+'
        r'(\w+)\s+'
        r'(\d+)\s+'
        r'---\s+'
        r'\[([^\]]+)\]\s+'
        r'([^\s:]+)\s*:?\s*'
        r'(.*)'
    )
    
    ERROR_KEYWORDS = [
        'Exception', 'Error', 'Failed', 'Fatal', 'Critical',
        'Timeout', 'Connection refused', 'OutOfMemoryError',
        'NullPointerException', 'StackOverflowError',
        'ClassNotFoundException', 'NoClassDefFoundError',
        'SQLException', 'IOException', 'SocketException'
    ]
    
    def __init__(self):
        self.parsed_logs = []
        
    def parse_log_line(self, line):
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        match = self.LOG_PATTERN.match(line.strip())
        if match:
            timestamp_str, level, pid, thread, class_path, message = match.groups()
            try:
                timestamp = pd.to_datetime(timestamp_str, format='%Y-%m-%d %H:%M:%S.%f')
            except:
                timestamp = pd.to_datetime(timestamp_str, errors='coerce')
            
            # ì—ëŸ¬ í‚¤ì›Œë“œ í™•ì¸
            is_error = level in ['ERROR', 'FATAL'] or any(
                keyword.lower() in message.lower() for keyword in self.ERROR_KEYWORDS
            )
            
            return {
                'timestamp': timestamp,
                'level': level,
                'pid': pid,
                'thread': thread.strip(),
                'class_path': class_path,
                'message': message,
                'is_error': is_error,
                'message_length': len(message),
                'has_exception': 'Exception' in message or 'Error' in message
            }
        return None
    
    def parse_log_file(self, file_path, max_lines=None):
        """
        ë¡œê·¸ íŒŒì¼ íŒŒì‹±
        
        Args:
            file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            max_lines: ìµœëŒ€ íŒŒì‹±í•  ë¼ì¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        logs = []
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    if max_lines and line_num > max_lines:
                        break
                    parsed = self.parse_log_line(line)
                    if parsed:
                        parsed['file_path'] = os.path.basename(file_path)
                        parsed['line_number'] = line_num
                        logs.append(parsed)
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
        return logs
    
    def parse_directory(self, directory_path, max_files=None, sample_lines=None):
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ë¡œê·¸ íŒŒì¼ íŒŒì‹±
        
        Args:
            directory_path: ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_files: ìµœëŒ€ íŒŒì‹±í•  íŒŒì¼ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            sample_lines: íŒŒì¼ë‹¹ ìµœëŒ€ íŒŒì‹±í•  ë¼ì¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        all_logs = []
        log_files = glob.glob(os.path.join(directory_path, '*.log'))
        
        if max_files:
            log_files = log_files[:max_files]
        
        print(f"ì´ {len(log_files)}ê°œ ë¡œê·¸ íŒŒì¼ ë°œê²¬")
        if max_files:
            print(f"  (ìµœëŒ€ {max_files}ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬)")
        
        for file_path in log_files:
            print(f"íŒŒì‹± ì¤‘: {os.path.basename(file_path)}")
            logs = self.parse_log_file(file_path, max_lines=sample_lines)
            all_logs.extend(logs)
            print(f"  - {len(logs)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
        
        return pd.DataFrame(all_logs)


class LogAnomalyDetector:
    """ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.baseline_stats = {}
        self.scaler = StandardScaler()
        self.models = {}
        
    def extract_features(self, df):
        """ë¡œê·¸ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        if df.empty:
            return pd.DataFrame()
        
        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
        df['hour'] = df['timestamp'].dt.hour
        df['minute'] = df['timestamp'].dt.minute
        df['date'] = df['timestamp'].dt.date
        
        # ì‹œê°„ ìœˆë„ìš°ë³„ ì§‘ê³„ (10ë¶„ ë‹¨ìœ„)
        df['time_window'] = df['timestamp'].dt.floor('10T')
        
        # ì§‘ê³„
        features = []
        for window in df['time_window'].unique():
            window_df = df[df['time_window'] == window]
            
            feature = {
                'time_window': window,
                'total_logs': len(window_df),
                'error_count': window_df['is_error'].sum(),
                'warn_count': (window_df['level'] == 'WARN').sum(),
                'error_rate': window_df['is_error'].mean(),
                'warn_rate': (window_df['level'] == 'WARN').mean(),
                'unique_classes': window_df['class_path'].nunique(),
                'unique_threads': window_df['thread'].nunique(),
                'avg_message_length': window_df['message_length'].mean(),
                'exception_count': window_df['has_exception'].sum(),
                'exception_rate': window_df['has_exception'].mean(),
                'unique_files': window_df['file_path'].nunique(),
            }
            
            # ë ˆë²¨ë³„ ì¹´ìš´íŠ¸
            level_counts = window_df['level'].value_counts()
            for level in ['ERROR', 'WARN', 'INFO', 'DEBUG']:
                feature[f'{level.lower()}_count'] = level_counts.get(level, 0)
            
            # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í´ë˜ìŠ¤
            top_class = window_df['class_path'].value_counts().head(1)
            if not top_class.empty:
                feature['top_class'] = top_class.index[0]
                feature['top_class_count'] = top_class.values[0]
            else:
                feature['top_class'] = ''
                feature['top_class_count'] = 0
            
            features.append(feature)
        
        features_df = pd.DataFrame(features)
        
        # í´ë˜ìŠ¤ ê²½ë¡œë¥¼ ìˆ«ìë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ)
        if 'top_class' in features_df.columns:
            features_df['top_class_hash'] = features_df['top_class'].apply(
                lambda x: hash(x) % 1000 if x else 0
            )
        
        return features_df
    
    def calculate_baseline(self, features_df):
        """ê¸°ì¤€ì„  í†µê³„ ê³„ì‚°"""
        if features_df.empty:
            return {}
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        baseline = {}
        
        for col in numeric_cols:
            baseline[f'{col}_mean'] = features_df[col].mean()
            baseline[f'{col}_std'] = features_df[col].std()
            baseline[f'{col}_median'] = features_df[col].median()
            baseline[f'{col}_q25'] = features_df[col].quantile(0.25)
            baseline[f'{col}_q75'] = features_df[col].quantile(0.75)
            baseline[f'{col}_q95'] = features_df[col].quantile(0.95)
        
        return baseline
    
    def detect_statistical_anomalies(self, features_df, threshold=3.0):
        """í†µê³„ì  ì´ìƒì¹˜ íƒì§€ (Z-score ê¸°ë°˜)"""
        if features_df.empty or not self.baseline_stats:
            return pd.DataFrame()
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        anomalies = []
        
        for idx, row in features_df.iterrows():
            anomaly_score = 0
            reasons = []
            
            for col in numeric_cols:
                mean_key = f'{col}_mean'
                std_key = f'{col}_std'
                
                if mean_key in self.baseline_stats and std_key in self.baseline_stats:
                    mean_val = self.baseline_stats[mean_key]
                    std_val = self.baseline_stats[std_key]
                    
                    if std_val > 0:
                        z_score = abs((row[col] - mean_val) / std_val)
                        if z_score > threshold:
                            anomaly_score += z_score
                            reasons.append(f"{col}: Z-score={z_score:.2f}")
            
            if anomaly_score > 0:
                anomalies.append({
                    'time_window': row['time_window'],
                    'anomaly_score': anomaly_score,
                    'reasons': '; '.join(reasons),
                    'features': row.to_dict()
                })
        
        return pd.DataFrame(anomalies)
    
    def detect_error_spikes(self, features_df, threshold_multiplier=5.0):
        """ì—ëŸ¬ ê¸‰ì¦ íƒì§€"""
        if features_df.empty or not self.baseline_stats:
            return pd.DataFrame()
        
        baseline_error_rate = self.baseline_stats.get('error_rate_mean', 0)
        baseline_error_std = self.baseline_stats.get('error_rate_std', 0)
        
        if baseline_error_rate == 0:
            baseline_error_rate = 0.01  # ìµœì†Œê°’
        
        spikes = []
        for idx, row in features_df.iterrows():
            current_error_rate = row['error_rate']
            
            if current_error_rate > baseline_error_rate * threshold_multiplier:
                spikes.append({
                    'time_window': row['time_window'],
                    'baseline_error_rate': baseline_error_rate,
                    'current_error_rate': current_error_rate,
                    'multiplier': current_error_rate / baseline_error_rate,
                    'error_count': row['error_count'],
                    'total_logs': row['total_logs']
                })
        
        return pd.DataFrame(spikes)
    
    def detect_unusual_patterns(self, df):
        """ë¹„ì •ìƒì ì¸ íŒ¨í„´ íƒì§€"""
        anomalies = []
        
        # 1. íŠ¹ì • í´ë˜ìŠ¤ì—ì„œ ì—ëŸ¬ ì§‘ì¤‘
        error_by_class = df[df['is_error']].groupby('class_path').size()
        if not error_by_class.empty:
            top_error_class = error_by_class.idxmax()
            error_count = error_by_class.max()
            total_errors = error_by_class.sum()
            
            if error_count > total_errors * 0.5:  # ì „ì²´ ì—ëŸ¬ì˜ 50% ì´ìƒì´ í•œ í´ë˜ìŠ¤ì—ì„œ
                anomalies.append({
                    'type': 'error_concentration',
                    'class': top_error_class,
                    'error_count': error_count,
                    'total_errors': total_errors,
                    'percentage': (error_count / total_errors) * 100
                })
        
        # 2. ë¡œê·¸ ë¹ˆë„ ì´ìƒ (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìŒ)
        if 'time_window' in df.columns:
            log_frequency = df.groupby('time_window').size()
            if not log_frequency.empty:
                mean_freq = log_frequency.mean()
                std_freq = log_frequency.std()
                
                for window, count in log_frequency.items():
                    if std_freq > 0:
                        z_score = abs((count - mean_freq) / std_freq)
                        if z_score > 3:
                            anomalies.append({
                                'type': 'frequency_anomaly',
                                'time_window': window,
                                'log_count': count,
                                'mean': mean_freq,
                                'z_score': z_score
                            })
        
        # 3. ìƒˆë¡œìš´ ì˜ˆì™¸ íƒ€ì… íƒì§€
        exception_patterns = df[df['has_exception']]['message'].apply(
            lambda x: re.search(r'(\w+Exception|\w+Error)', x)
        )
        exception_types = exception_patterns.dropna().apply(lambda x: x.group(1))
        
        if not exception_types.empty:
            exception_counts = exception_types.value_counts()
            # ì „ì²´ì˜ 1% ë¯¸ë§Œì´ë©´ ìƒˆë¡œìš´ ì˜ˆì™¸ë¡œ ê°„ì£¼
            total_exceptions = len(exception_types)
            for exc_type, count in exception_counts.items():
                if count < total_exceptions * 0.01 and count > 0:
                    anomalies.append({
                        'type': 'new_exception_type',
                        'exception_type': exc_type,
                        'count': count,
                        'percentage': (count / total_exceptions) * 100
                    })
        
        return pd.DataFrame(anomalies)
    
    def train_ml_model(self, features_df, model_type='isolation_forest'):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"""
        if features_df.empty:
            return None
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        X = features_df[numeric_cols].fillna(0)
        
        # ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X)
        
        # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
        if model_type == 'isolation_forest':
            model = IForest(contamination=0.1, random_state=42)
        elif model_type == 'autoencoder':
            # AutoEncoderëŠ” ë°ì´í„° í¬ê¸°ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”
            n_samples, n_features = X_scaled.shape
            
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ AutoEncoder ì‚¬ìš© ë¶ˆê°€
            if n_samples < 10:
                print(f"âš ï¸ AutoEncoder í•™ìŠµ ì‹¤íŒ¨: ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({n_samples}ê°œ ìƒ˜í”Œ)")
                print(f"   ìµœì†Œ 10ê°œ ì´ìƒì˜ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return None
            
            # íŠ¹ì§• ìˆ˜ì— ë”°ë¼ hidden layer í¬ê¸° ì¡°ì •
            if n_features <= 5:
                hidden_neurons = [max(4, n_features), max(2, n_features//2), max(4, n_features)]
            elif n_features <= 10:
                hidden_neurons = [16, 8, 16]
            else:
                hidden_neurons = [64, 32, 16, 32, 64]
            
            # ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ epochì™€ batch_size ì¡°ì •
            if n_samples < 50:
                epoch_num = 20
                batch_size = min(8, n_samples)
            elif n_samples < 100:
                epoch_num = 30
                batch_size = 16
            else:
                epoch_num = 50
                batch_size = 32
            
            try:
                model = AutoEncoder(
                    contamination=0.1,
                    hidden_neurons=hidden_neurons,
                    epochs=epoch_num,
                    batch_size=batch_size,
                    dropout_rate=0.2,
                    verbose=0,  # ì§„í–‰ ìƒí™© ì¶œë ¥ ë¹„í™œì„±í™”
                    random_state=42
                )
            except TypeError:
                # íŒŒë¼ë¯¸í„° ì´ë¦„ì´ ë‹¤ë¥¸ ë²„ì „ì˜ pyodì¼ ìˆ˜ ìˆìŒ
                try:
                    model = AutoEncoder(
                        contamination=0.1,
                        hidden_neuron_list=hidden_neurons,
                        epoch_num=epoch_num,
                        batch_size=batch_size,
                        dropout_rate=0.2,
                        verbose=0,
                        random_state=42
                    )
                except Exception as e:
                    print(f"âš ï¸ AutoEncoder ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return None
        elif model_type == 'lof':
            model = LOF(contamination=0.1)
        else:
            model = IForest(contamination=0.1, random_state=42)
        
        try:
            model.fit(X_scaled)
            self.models[model_type] = model
            return model
        except Exception as e:
            print(f"âš ï¸ {model_type} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            print(f"   ë°ì´í„° í¬ê¸°: {X_scaled.shape}")
            print(f"   ëª¨ë¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
    
    def predict_anomalies_ml(self, features_df, model_type='isolation_forest'):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ì´ìƒì¹˜ ì˜ˆì¸¡"""
        if features_df.empty or model_type not in self.models:
            return pd.DataFrame()
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        X = features_df[numeric_cols].fillna(0)
        X_scaled = self.scaler.transform(X)
        
        model = self.models[model_type]
        predictions = model.predict(X_scaled)
        scores = model.decision_function(X_scaled)
        
        anomalies = features_df[predictions == 1].copy()
        anomalies['anomaly_score'] = -scores[predictions == 1]  # ìŒìˆ˜ ì ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ
        
        return anomalies


class LogAnomalyDetectionSystem:
    """í†µí•© ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_directory, max_files=None, sample_lines=None):
        self.log_directory = log_directory
        self.max_files = max_files
        self.sample_lines = sample_lines
        self.parser = SpringBootLogParser()
        self.detector = LogAnomalyDetector()
        self.logs_df = None
        self.features_df = None
        
    def load_logs(self):
        """ë¡œê·¸ íŒŒì¼ ë¡œë“œ ë° íŒŒì‹±"""
        print("=" * 60)
        print("ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘...")
        print("=" * 60)
        
        self.logs_df = self.parser.parse_directory(
            self.log_directory, 
            max_files=self.max_files,
            sample_lines=self.sample_lines
        )
        
        if self.logs_df.empty:
            print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nâœ… ì´ {len(self.logs_df)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
        print(f"   - ê¸°ê°„: {self.logs_df['timestamp'].min()} ~ {self.logs_df['timestamp'].max()}")
        print(f"   - ì—ëŸ¬ ë¡œê·¸: {self.logs_df['is_error'].sum()}ê°œ")
        print(f"   - ê²½ê³  ë¡œê·¸: {(self.logs_df['level'] == 'WARN').sum()}ê°œ")
        
        return True
    
    def extract_features(self):
        """íŠ¹ì§• ì¶”ì¶œ"""
        print("\n" + "=" * 60)
        print("íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        print("=" * 60)
        
        self.features_df = self.detector.extract_features(self.logs_df)
        
        if self.features_df.empty:
            print("âš ï¸ ì¶”ì¶œëœ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… {len(self.features_df)}ê°œ ì‹œê°„ ìœˆë„ìš° íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        print(f"\níŠ¹ì§• í†µê³„:")
        print(self.features_df.describe())
        
        return True
    
    def train_baseline(self, train_ratio=0.8, validation_ratio=0.1):
        """
        ê¸°ì¤€ì„  í•™ìŠµ (80% í•™ìŠµ, 10% ê²€ì¦, 10% í…ŒìŠ¤íŠ¸)
        
        Args:
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.8 = 80%)
            validation_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.1 = 10%, ë‚˜ë¨¸ì§€ 10%ëŠ” í…ŒìŠ¤íŠ¸)
        """
        print("\n" + "=" * 60)
        print("ê¸°ì¤€ì„  í•™ìŠµ ì¤‘...")
        print("=" * 60)
        
        if self.features_df.empty:
            print("âš ï¸ íŠ¹ì§• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ë¶„í• 
        self.features_df = self.features_df.sort_values('time_window')
        total_samples = len(self.features_df)
        
        # 80% í•™ìŠµ, 10% ê²€ì¦, 10% í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• 
        train_end = int(total_samples * train_ratio)
        val_end = train_end + int(total_samples * validation_ratio)
        
        train_df = self.features_df.iloc[:train_end]
        val_df = self.features_df.iloc[train_end:val_end]
        test_df = self.features_df.iloc[val_end:]
        
        # ê¸°ì¤€ì„  í†µê³„ ê³„ì‚° (í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš©)
        self.detector.baseline_stats = self.detector.calculate_baseline(train_df)
        
        print("âœ… ê¸°ì¤€ì„  í†µê³„ ê³„ì‚° ì™„ë£Œ")
        print(f"   - ì „ì²´ ë°ì´í„°: {total_samples}ê°œ ìœˆë„ìš°")
        print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ìœˆë„ìš° ({len(train_df)/total_samples*100:.1f}%)")
        print(f"   - ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ ìœˆë„ìš° ({len(val_df)/total_samples*100:.1f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ ìœˆë„ìš° ({len(test_df)/total_samples*100:.1f}%)")
        
        # ML ëª¨ë¸ í•™ìŠµ
        print("\në¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        models_trained = {}
        
        if self.detector.train_ml_model(train_df, model_type='isolation_forest'):
            print("   âœ… Isolation Forest í•™ìŠµ ì™„ë£Œ")
            models_trained['isolation_forest'] = True
        else:
            print("   âš ï¸ Isolation Forest í•™ìŠµ ì‹¤íŒ¨")
            models_trained['isolation_forest'] = False
        
        if self.detector.train_ml_model(train_df, model_type='autoencoder'):
            print("   âœ… AutoEncoder í•™ìŠµ ì™„ë£Œ")
            models_trained['autoencoder'] = True
        else:
            print("   âš ï¸ AutoEncoder í•™ìŠµ ì‹¤íŒ¨ (ê±´ë„ˆëœ€)")
            models_trained['autoencoder'] = False
        
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
        print("\n" + "=" * 60)
        print("ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 60)
        
        validation_results = self._evaluate_models(val_df, models_trained)
        self._print_validation_results(validation_results)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ (ë‚˜ì¤‘ì— ì‚¬ìš©)
        self.test_df = test_df
        
        return True
    
    def _evaluate_models(self, val_df, models_trained):
        """ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        results = {}
        
        # ì •ìƒ/ì´ìƒ ë¼ë²¨ ìƒì„± (ì—ëŸ¬ìœ¨ ê¸°ì¤€)
        baseline_error_rate = self.detector.baseline_stats.get('error_rate_mean', 0)
        threshold = baseline_error_rate * 2  # ê¸°ì¤€ ì—ëŸ¬ìœ¨ì˜ 2ë°° ì´ìƒì´ë©´ ì´ìƒ
        
        val_df = val_df.copy()
        val_df['true_label'] = (val_df['error_rate'] > threshold).astype(int)
        
        # ê° ëª¨ë¸ë³„ í‰ê°€
        for model_type, is_trained in models_trained.items():
            if not is_trained or model_type not in self.detector.models:
                continue
            
            try:
                # ì˜ˆì¸¡
                predictions = self.detector.predict_anomalies_ml(val_df, model_type=model_type)
                
                if predictions.empty:
                    continue
                
                # ì˜ˆì¸¡ ë¼ë²¨ ìƒì„± (ì´ìƒì¹˜ë¡œ íƒì§€ëœ ê²ƒ)
                val_df['pred_label'] = 0
                val_df.loc[predictions.index, 'pred_label'] = 1
                
                # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                true_labels = val_df['true_label'].values
                pred_labels = val_df['pred_label'].values
                
                # ë¼ë²¨ì´ ëª¨ë‘ ê°™ìœ¼ë©´ í‰ê°€ ë¶ˆê°€
                if len(set(true_labels)) == 1 and len(set(pred_labels)) == 1:
                    continue
                
                accuracy = accuracy_score(true_labels, pred_labels)
                precision = precision_score(true_labels, pred_labels, zero_division=0)
                recall = recall_score(true_labels, pred_labels, zero_division=0)
                f1 = f1_score(true_labels, pred_labels, zero_division=0)
                cm = confusion_matrix(true_labels, pred_labels)
                
                results[model_type] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'confusion_matrix': cm,
                    'true_anomalies': int(true_labels.sum()),
                    'predicted_anomalies': int(pred_labels.sum())
                }
            except Exception as e:
                print(f"   âš ï¸ {model_type} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        return results
    
    def _print_validation_results(self, validation_results):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        if not validation_results:
            print("âš ï¸ í‰ê°€ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        for model_type, metrics in validation_results.items():
            print(f"\nğŸ“Š {model_type.upper()} ëª¨ë¸ ì„±ëŠ¥:")
            print(f"   ì •í™•ë„ (Accuracy): {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
            print(f"   ì •ë°€ë„ (Precision): {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
            print(f"   ì¬í˜„ìœ¨ (Recall): {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
            print(f"   F1 ì ìˆ˜: {metrics['f1_score']:.4f}")
            print(f"   ì‹¤ì œ ì´ìƒì¹˜: {metrics['true_anomalies']}ê°œ")
            print(f"   ì˜ˆì¸¡ ì´ìƒì¹˜: {metrics['predicted_anomalies']}ê°œ")
            
            cm = metrics['confusion_matrix']
            print(f"   í˜¼ë™ í–‰ë ¬:")
            print(f"      ì •ìƒâ†’ì •ìƒ: {cm[0][0]}, ì •ìƒâ†’ì´ìƒ: {cm[0][1]}")
            print(f"      ì´ìƒâ†’ì •ìƒ: {cm[1][0]}, ì´ìƒâ†’ì´ìƒ: {cm[1][1]}")
    
    def detect_all_anomalies(self, use_test_data=True):
        """
        ëª¨ë“  ì´ìƒì¹˜ íƒì§€
        
        Args:
            use_test_data: Trueë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©, Falseë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
        """
        print("\n" + "=" * 60)
        print("ì´ìƒì¹˜ íƒì§€ ì¤‘...")
        print("=" * 60)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì‚¬ìš©
        if use_test_data and hasattr(self, 'test_df') and not self.test_df.empty:
            print("ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì´ìƒì¹˜ íƒì§€ ìˆ˜í–‰")
            test_features_df = self.test_df
        else:
            print("ğŸ“ ì „ì²´ ë°ì´í„°ë¡œ ì´ìƒì¹˜ íƒì§€ ìˆ˜í–‰")
            test_features_df = self.features_df
        
        results = {}
        
        # 1. í†µê³„ì  ì´ìƒì¹˜
        print("\n1. í†µê³„ì  ì´ìƒì¹˜ íƒì§€...")
        stat_anomalies = self.detector.detect_statistical_anomalies(test_features_df)
        results['statistical'] = stat_anomalies
        print(f"   âœ… {len(stat_anomalies)}ê°œ ì´ìƒì¹˜ ë°œê²¬")
        
        # 2. ì—ëŸ¬ ê¸‰ì¦
        print("\n2. ì—ëŸ¬ ê¸‰ì¦ íƒì§€...")
        error_spikes = self.detector.detect_error_spikes(test_features_df)
        results['error_spikes'] = error_spikes
        print(f"   âœ… {len(error_spikes)}ê°œ ì—ëŸ¬ ê¸‰ì¦ ë°œê²¬")
        
        # 3. ë¹„ì •ìƒ íŒ¨í„´ (ì „ì²´ ë¡œê·¸ ë°ì´í„° ì‚¬ìš©)
        print("\n3. ë¹„ì •ìƒ íŒ¨í„´ íƒì§€...")
        unusual_patterns = self.detector.detect_unusual_patterns(self.logs_df)
        results['unusual_patterns'] = unusual_patterns
        print(f"   âœ… {len(unusual_patterns)}ê°œ ë¹„ì •ìƒ íŒ¨í„´ ë°œê²¬")
        
        # 4. ML ê¸°ë°˜ ì´ìƒì¹˜
        print("\n4. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€...")
        ml_anomalies_if = self.detector.predict_anomalies_ml(
            test_features_df, model_type='isolation_forest'
        )
        results['ml_isolation_forest'] = ml_anomalies_if
        print(f"   âœ… Isolation Forest: {len(ml_anomalies_if)}ê°œ ì´ìƒì¹˜")
        
        # AutoEncoderëŠ” í•™ìŠµì´ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
        if 'autoencoder' in self.detector.models:
            ml_anomalies_ae = self.detector.predict_anomalies_ml(
                test_features_df, model_type='autoencoder'
            )
            results['ml_autoencoder'] = ml_anomalies_ae
            print(f"   âœ… AutoEncoder: {len(ml_anomalies_ae)}ê°œ ì´ìƒì¹˜")
        else:
            results['ml_autoencoder'] = pd.DataFrame()
            print(f"   âš ï¸ AutoEncoder: í•™ìŠµë˜ì§€ ì•Šì•„ ê±´ë„ˆëœ€")
        
        return results
    
    def save_model(self, model_path):
        """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¤€ì„  ì €ì¥"""
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        model_data = {
            'baseline_stats': self.detector.baseline_stats,
            'scaler': self.detector.scaler,
            'models': self.detector.models
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    
    def load_model(self, model_path):
        """ì €ì¥ëœ ëª¨ë¸ê³¼ ê¸°ì¤€ì„  ë¡œë“œ"""
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        self.detector.baseline_stats = model_data['baseline_stats']
        self.detector.scaler = model_data['scaler']
        self.detector.models = model_data['models']
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def detect_anomalies_on_new_data(self, new_log_directory, max_files=None, sample_lines=None):
        """
        ìƒˆë¡œìš´ ë¡œê·¸ ë°ì´í„°ì— ëŒ€í•´ ì´ìƒì¹˜ íƒì§€
        
        Args:
            new_log_directory: ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_files: ìµœëŒ€ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
            sample_lines: íŒŒì¼ë‹¹ ìµœëŒ€ ì²˜ë¦¬í•  ë¼ì¸ ìˆ˜
        
        Returns:
            dict: ì´ìƒì¹˜ íƒì§€ ê²°ê³¼
        """
        print("=" * 60)
        print("ìƒˆë¡œìš´ ë¡œê·¸ ë°ì´í„° ì´ìƒì¹˜ íƒì§€")
        print("=" * 60)
        
        # ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì‹±
        print("\nìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì¤‘...")
        new_logs_df = self.parser.parse_directory(
            new_log_directory,
            max_files=max_files,
            sample_lines=sample_lines
        )
        
        if new_logs_df.empty:
            print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(new_logs_df)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
        
        # íŠ¹ì§• ì¶”ì¶œ
        print("\níŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        new_features_df = self.detector.extract_features(new_logs_df)
        
        if new_features_df.empty:
            print("âš ï¸ ì¶”ì¶œëœ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(new_features_df)}ê°œ ì‹œê°„ ìœˆë„ìš° íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        
        # ì´ìƒì¹˜ íƒì§€
        print("\nì´ìƒì¹˜ íƒì§€ ì¤‘...")
        results = {}
        
        # 1. í†µê³„ì  ì´ìƒì¹˜
        stat_anomalies = self.detector.detect_statistical_anomalies(new_features_df)
        results['statistical'] = stat_anomalies
        print(f"   âœ… í†µê³„ì  ì´ìƒì¹˜: {len(stat_anomalies)}ê°œ")
        
        # 2. ì—ëŸ¬ ê¸‰ì¦
        error_spikes = self.detector.detect_error_spikes(new_features_df)
        results['error_spikes'] = error_spikes
        print(f"   âœ… ì—ëŸ¬ ê¸‰ì¦: {len(error_spikes)}ê°œ")
        
        # 3. ë¹„ì •ìƒ íŒ¨í„´
        unusual_patterns = self.detector.detect_unusual_patterns(new_logs_df)
        results['unusual_patterns'] = unusual_patterns
        print(f"   âœ… ë¹„ì •ìƒ íŒ¨í„´: {len(unusual_patterns)}ê°œ")
        
        # 4. ML ê¸°ë°˜ ì´ìƒì¹˜
        if 'isolation_forest' in self.detector.models:
            ml_anomalies_if = self.detector.predict_anomalies_ml(
                new_features_df, model_type='isolation_forest'
            )
            results['ml_isolation_forest'] = ml_anomalies_if
            print(f"   âœ… ML ì´ìƒì¹˜ (IF): {len(ml_anomalies_if)}ê°œ")
        else:
            results['ml_isolation_forest'] = pd.DataFrame()
            print(f"   âš ï¸ ML ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•ŠìŒ")
        
        if 'autoencoder' in self.detector.models:
            ml_anomalies_ae = self.detector.predict_anomalies_ml(
                new_features_df, model_type='autoencoder'
            )
            results['ml_autoencoder'] = ml_anomalies_ae
            print(f"   âœ… ML ì´ìƒì¹˜ (AE): {len(ml_anomalies_ae)}ê°œ")
        else:
            results['ml_autoencoder'] = pd.DataFrame()
        
        return results
    
    def generate_report(self, results):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 60)
        print("ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # í†µê³„ì  ì´ìƒì¹˜
        if not results['statistical'].empty:
            print("\nğŸ“Š í†µê³„ì  ì´ìƒì¹˜:")
            for idx, row in results['statistical'].head(10).iterrows():
                print(f"   ì‹œê°„: {row['time_window']}")
                print(f"   ì´ìƒ ì ìˆ˜: {row['anomaly_score']:.2f}")
                print(f"   ì´ìœ : {row['reasons']}")
                print()
        
        # ì—ëŸ¬ ê¸‰ì¦
        if not results['error_spikes'].empty:
            print("\nğŸš¨ ì—ëŸ¬ ê¸‰ì¦:")
            for idx, row in results['error_spikes'].head(10).iterrows():
                print(f"   ì‹œê°„: {row['time_window']}")
                print(f"   ê¸°ì¤€ ì—ëŸ¬ìœ¨: {row['baseline_error_rate']:.2%}")
                print(f"   í˜„ì¬ ì—ëŸ¬ìœ¨: {row['current_error_rate']:.2%}")
                print(f"   ë°°ìˆ˜: {row['multiplier']:.1f}ë°°")
                print(f"   ì—ëŸ¬ ìˆ˜: {row['error_count']}ê°œ / ì´ {row['total_logs']}ê°œ")
                print()
        
        # ë¹„ì •ìƒ íŒ¨í„´
        if not results['unusual_patterns'].empty:
            print("\nâš ï¸ ë¹„ì •ìƒ íŒ¨í„´:")
            for idx, row in results['unusual_patterns'].iterrows():
                if row['type'] == 'error_concentration':
                    print(f"   ì—ëŸ¬ ì§‘ì¤‘: {row['class']}ì—ì„œ {row['error_count']}ê°œ ({row['percentage']:.1f}%)")
                elif row['type'] == 'frequency_anomaly':
                    print(f"   ë¡œê·¸ ë¹ˆë„ ì´ìƒ: {row['time_window']} (Z-score: {row['z_score']:.2f})")
                elif row['type'] == 'new_exception_type':
                    print(f"   ìƒˆë¡œìš´ ì˜ˆì™¸: {row['exception_type']} ({row['count']}íšŒ)")
                print()
        
        # ML ê¸°ë°˜ ì´ìƒì¹˜
        if not results['ml_isolation_forest'].empty:
            print("\nğŸ¤– ML ê¸°ë°˜ ì´ìƒì¹˜ (Isolation Forest):")
            for idx, row in results['ml_isolation_forest'].head(10).iterrows():
                print(f"   ì‹œê°„: {row['time_window']}")
                print(f"   ì´ìƒ ì ìˆ˜: {row['anomaly_score']:.2f}")
                print(f"   ì—ëŸ¬ ìˆ˜: {row['error_count']}ê°œ")
                print()
        
        # ìš”ì•½
        print("\n" + "=" * 60)
        print("ìš”ì•½")
        print("=" * 60)
        print(f"í†µê³„ì  ì´ìƒì¹˜: {len(results['statistical'])}ê°œ")
        print(f"ì—ëŸ¬ ê¸‰ì¦: {len(results['error_spikes'])}ê°œ")
        print(f"ë¹„ì •ìƒ íŒ¨í„´: {len(results['unusual_patterns'])}ê°œ")
        print(f"ML ì´ìƒì¹˜ (IF): {len(results['ml_isolation_forest'])}ê°œ")
        print(f"ML ì´ìƒì¹˜ (AE): {len(results['ml_autoencoder'])}ê°œ")
        
        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    # ìƒ˜í”Œë§ ì˜µì…˜ (ì „ì²´ ë¶„ì„ì„ ì›í•˜ë©´ Noneìœ¼ë¡œ ì„¤ì •)
    MAX_FILES = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´: 5ê°œë§Œ ì²˜ë¦¬)
    SAMPLE_LINES = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì „ì²´ ë¼ì¸ ì²˜ë¦¬ (ê¸°ì¡´: 10000ì¤„ë§Œ ì²˜ë¦¬)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = LogAnomalyDetectionSystem(
        log_directory,
        max_files=MAX_FILES,
        sample_lines=SAMPLE_LINES
    )
    
    # ë¡œê·¸ ë¡œë“œ
    if not system.load_logs():
        return
    
    # íŠ¹ì§• ì¶”ì¶œ
    if not system.extract_features():
        return
    
    # ê¸°ì¤€ì„  í•™ìŠµ
    if not system.train_baseline():
        return
    
    # ì´ìƒì¹˜ íƒì§€ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©)
    results = system.detect_all_anomalies(use_test_data=True)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    system.generate_report(results)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results"
    os.makedirs(output_dir, exist_ok=True)
    
    for name, df in results.items():
        if not df.empty:
            output_path = os.path.join(output_dir, f"anomalies_{name}.csv")
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ëª¨ë¸ ì €ì¥
    model_path = os.path.join(output_dir, "trained_model.pkl")
    system.save_model(model_path)


def test_new_logs():
    """ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ì— ëŒ€í•´ ì´ìƒì¹˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
    # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    model_path = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results/trained_model.pkl"
    
    # ìƒˆë¡œìš´ ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì˜ˆì‹œ)
    new_log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¨¼ì € í•™ìŠµ í•„ìš”
    if not os.path.exists(model_path):
        print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € main()ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        return
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = LogAnomalyDetectionSystem(new_log_directory)
    
    # ëª¨ë¸ ë¡œë“œ
    system.load_model(model_path)
    
    # ìƒˆë¡œìš´ ë¡œê·¸ ë°ì´í„°ë¡œ ì´ìƒì¹˜ íƒì§€
    # ì˜ˆ: ìµœê·¼ 3ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
    results = system.detect_anomalies_on_new_data(
        new_log_directory,
        max_files=3,  # ì²˜ìŒ 3ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
        sample_lines=5000  # íŒŒì¼ë‹¹ 5000ì¤„ë§Œ ì²˜ë¦¬
    )
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    system.generate_report(results)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results"
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    for name, df in results.items():
        if not df.empty:
            output_path = os.path.join(output_dir, f"test_anomalies_{name}_{timestamp}.csv")
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒˆë¡œìš´ ë¡œê·¸ì— ëŒ€í•´ ì´ìƒì¹˜ íƒì§€
        test_new_logs()
    else:
        # í•™ìŠµ ëª¨ë“œ: ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
        main()

